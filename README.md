# gitOps-workflow-flux

**Background -**
This repository contains my implementation of an SRE Challenge designed to demonstrate end-to-end skills in Kubernetes provisioning, 
GitOps workflows, application deployment, monitoring, and logging using Infrastructure as Code (IaC) principles.
The goal is to simulate a real-world Site Reliability Engineering scenario where an engineer is responsible for:
- Provisioning a secure and production-ready Kubernetes cluster using Terraform.
- Bootstrapping Flux for GitOps-based deployment automation.
- Deploying a sample web application with automated upgrades and rollbacks.
- Setting up basic monitoring (Prometheus, Grafana) and centralized logging (Fluentd).
- Following security, scalability, and resilience best practices.

This challenge emphasizes:
**Automation â€”** All components can be deployed or destroyed without manual intervention.
**Security-first design â€”** RBAC, Network Policies, and restricted Pod permissions.
**Operational excellence â€”** Observability and fault-tolerance are built in from the start.

The documentation in this repository captures the steps taken, the issues encountered, and the design choices made â€” making it reproducible and extensible for future improvements.

## ğŸ—ï¸ Repository Structure
```
â”œâ”€â”€ apps
â”‚Â Â  â””â”€â”€ staging
â”‚Â Â      â””â”€â”€ sre-challenge-app
â”‚Â Â          â”œâ”€â”€ configmap.yaml
â”‚Â Â          â”œâ”€â”€ deployment.yaml
â”‚Â Â          â”œâ”€â”€ gitrepository.yaml
â”‚Â Â          â”œâ”€â”€ health-monitor.yaml
â”‚Â Â          â”œâ”€â”€ hpa.yaml
â”‚Â Â          â”œâ”€â”€ imagepolicy.yaml
â”‚Â Â          â”œâ”€â”€ imagerepository.yaml
â”‚Â Â          â”œâ”€â”€ imageupdateautomation.yaml
â”‚Â Â          â”œâ”€â”€ ingress.yaml
â”‚Â Â          â”œâ”€â”€ kustomization-flux.yaml
â”‚Â Â          â”œâ”€â”€ kustomization.yaml
â”‚Â Â          â”œâ”€â”€ namespace.yaml              # App namespace
â”‚Â Â          â””â”€â”€ service.yaml
â”œâ”€â”€ clusters
â”‚Â Â  â””â”€â”€ staging
â”‚Â Â      â”œâ”€â”€ flux-system
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ gotk-components.yaml        # Flux controllers
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ gotk-sync.yaml              # Git repository sync
â”‚Â Â      â”‚Â Â  â””â”€â”€ kustomization.yaml          # Flux system config
â”‚Â Â      â”œâ”€â”€ image-automation.yaml
â”‚Â Â      â””â”€â”€ kustomization.yaml              # Cluster-level config
â”œâ”€â”€ infrastructure
â”‚Â Â  â””â”€â”€ staging
â”‚Â Â      â”œâ”€â”€ aws-load-balancer-controller
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ crds.yaml
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ deployment.yaml
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ kustomization.yaml
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ namespace.yaml
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ rbac.yaml
â”‚Â Â      â”‚Â Â  â”œâ”€â”€ serviceaccount.yaml
â”‚Â Â      â”‚Â Â  â””â”€â”€ webhook.yaml
â”‚Â Â      â”œâ”€â”€ kustomization.yaml
â”‚Â Â      â””â”€â”€ monitoring
â”‚Â Â          â”œâ”€â”€ application-health-dashboard.yaml
â”‚Â Â          â”œâ”€â”€ fluent-bit.yaml
â”‚Â Â          â”œâ”€â”€ kustomization.yaml
â”‚Â Â          â”œâ”€â”€ namespace.yaml
â”‚Â Â          â”œâ”€â”€ node-health-dashboard.yaml
â”‚Â Â          â”œâ”€â”€ prometheus-operator.yaml
â”‚Â Â          â”œâ”€â”€ servicemonitors.yaml
â”‚Â Â          â”œâ”€â”€ simple-ingress.yaml
â”‚Â Â          â””â”€â”€ sre-challenge-dashboard.yaml
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts
â”‚Â Â  â”œâ”€â”€ deploy-global-regional.sh         # Terraform deployment script
â”‚Â Â  â””â”€â”€ install-flux.sh                   # Flux bootstrap script
â””â”€â”€ terraform
    â”œâ”€â”€ environments
    â”‚Â Â  â”œâ”€â”€ production
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ global
    â”‚Â Â  â”‚Â Â  â”‚Â Â  â””â”€â”€ terraform.tfvars
    â”‚Â Â  â”‚Â Â  â””â”€â”€ regional
    â”‚Â Â  â”‚Â Â      â”œâ”€â”€ eu-central-1
    â”‚Â Â  â”‚Â Â      â”‚Â Â  â”œâ”€â”€ main.tf
    â”‚Â Â  â”‚Â Â      â”‚Â Â  â”œâ”€â”€ outputs.tf
    â”‚Â Â  â”‚Â Â      â”‚Â Â  â”œâ”€â”€ terraform.tfvars
    â”‚Â Â  â”‚Â Â      â”‚Â Â  â””â”€â”€ variables.tf
    â”‚Â Â  â”‚Â Â      â””â”€â”€ us-east-1
    â”‚Â Â  â”‚Â Â          â”œâ”€â”€ main.tf
    â”‚Â Â  â”‚Â Â          â”œâ”€â”€ outputs.tf
    â”‚Â Â  â”‚Â Â          â”œâ”€â”€ terraform.tfvars
    â”‚Â Â  â”‚Â Â          â””â”€â”€ variables.tf
    â”‚Â Â  â””â”€â”€ staging
    â”‚Â Â      â”œâ”€â”€ global
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ backend-config.hcl
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ backend.hcl
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ main.tf
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ outputs.tf
    â”‚Â Â      â”‚Â Â  â”œâ”€â”€ terraform.tfvars
    â”‚Â Â      â”‚Â Â  â””â”€â”€ variables.tf
    â”‚Â Â      â””â”€â”€ regional
    â”‚Â Â          â”œâ”€â”€ eu-central-1
    â”‚Â Â          â”‚Â Â  â”œâ”€â”€ backend-config.hcl
    â”‚Â Â          â”‚Â Â  â”œâ”€â”€ main.tf
    â”‚Â Â          â”‚Â Â  â”œâ”€â”€ outputs.tf
    â”‚Â Â          â”‚Â Â  â”œâ”€â”€ terraform.tfvars
    â”‚Â Â          â”‚Â Â  â””â”€â”€ variables.tf
    â”‚Â Â          â””â”€â”€ us-east-1
    â”‚Â Â              â”œâ”€â”€ main.tf
    â”‚Â Â              â”œâ”€â”€ outputs.tf
    â”‚Â Â              â”œâ”€â”€ terraform.tfvars
    â”‚Â Â              â””â”€â”€ variables.tf
    â”œâ”€â”€ modules
    â”‚Â Â  â”œâ”€â”€ acm
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
    â”‚Â Â  â”‚Â Â  â””â”€â”€ variables.tf
    â”‚Â Â  â”œâ”€â”€ alb
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
    â”‚Â Â  â”‚Â Â  â””â”€â”€ variables.tf
    â”‚Â Â  â”œâ”€â”€ eks
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
    â”‚Â Â  â”‚Â Â  â””â”€â”€ variables.tf
    â”‚Â Â  â”œâ”€â”€ iam
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
    â”‚Â Â  â”‚Â Â  â””â”€â”€ variables.tf
    â”‚Â Â  â”œâ”€â”€ iam-basic
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
    â”‚Â Â  â”‚Â Â  â””â”€â”€ variables.tf
    â”‚Â Â  â”œâ”€â”€ iam-irsa
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
    â”‚Â Â  â”‚Â Â  â””â”€â”€ variables.tf
    â”‚Â Â  â”œâ”€â”€ route53
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
    â”‚Â Â  â”‚Â Â  â””â”€â”€ variables.tf
    â”‚Â Â  â”œâ”€â”€ route53-records
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
    â”‚Â Â  â”‚Â Â  â””â”€â”€ variables.tf
    â”‚Â Â  â”œâ”€â”€ s3
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ main.tf
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ outputs.tf
    â”‚Â Â  â”‚Â Â  â””â”€â”€ variables.tf
    â”‚Â Â  â””â”€â”€ vpc
    â”‚Â Â      â”œâ”€â”€ main.tf
    â”‚Â Â      â”œâ”€â”€ outputs.tf
    â”‚Â Â      â””â”€â”€ variables.tf
    â””â”€â”€ README.md

```


# ğŸš€ Phase 1: Infrastructure Setup

## What Has Been Created

### ğŸ—ï¸ Terraform Modules
- **IAM Module**: EKS cluster roles, node group roles, AWS Load Balancer Controller, and Flux controller roles
- **VPC Module**: Multi-AZ VPC with public/private/database subnets, NAT gateways, security groups
- **EKS Module**: Managed Kubernetes cluster with node groups, addons, and OIDC provider
- **ALB Module**: Application Load Balancer with security groups and target groups
- **Route53 Module**: DNS management and SSL certificates via ACM
- **S3 Module**: Buckets for artifacts and configurations with lifecycle policies
- **ACM Module**: Automatic certificate management via ACM 


### ğŸŒ Multi-Region, Multi-Environment Structure
```
Staging Account (215876814712):
â”œâ”€â”€ us-east-1    (VPC: 10.0.0.0/16)
â””â”€â”€ eu-central-1 (VPC: 10.0.0.0/16)

Production Account (746848447423):
â”œâ”€â”€ us-east-1    (VPC: 10.10.0.0/16)
â””â”€â”€ eu-central-1 (VPC: 10.20.0.0/16)
```

### ğŸ”’ Security Features Implemented
- **Network Isolation**: Private subnets for EKS nodes, security groups with minimal access
- **RBAC**: Kubernetes role-based access control enabled
- **Encryption**: KMS keys for EKS secrets and S3 buckets
- **IAM**: Least privilege roles with OIDC integration
- **Monitoring**: CloudWatch logging and VPC flow logs

## Quick Start Deployment

### Prerequisites
1. AWS CLI configured with profiles:
   ```bash
   aws configure --profile staging    # Account: 215876814712
   aws configure --profile production # Account: 746848447423
   ```

2. Terraform >= 1.0 installed
3. kubectl installed

### Step 1: Deploy Infrastructure
```bash
# Deploy staging infrastructure
./scripts/deploy-global-regional.sh staging eu-central-1 apply # this is the operational region for this POC
./scripts/deploy-global-regional.sh staging us-east-1 apply # Nearly equivalent but not tested

# Deploy production infrastructure
./scripts/deploy-global-regional.sh production us-east-1 apply # Nearly equivalent but not tested
./scripts/deploy-global-regional.sh production eu-central-1 apply # Nearly equivalent but not tested
```

### Step 2: Configure kubectl
```bash
# Connect to staging cluster
aws eks update-kubeconfig --region us-east-1 --name sre-challenge-staging --profile staging

# Verify connection
kubectl cluster-info
kubectl get nodes
```

## Environment Configurations

### Staging Environment
- **Deletion Protection**: Disabled (easy cleanup)
- **Log Retention**: 7 days
- **Node Groups**: Includes spot instances for cost optimization
- **Force Destroy**: Enabled for development workflows
- **Domain**: sre-challenge-panther.network

### Production Environment
- **TODO**

## Architecture Overview

### Network Design
- **3-Tier Architecture**: Public, Private, Database subnets
- **Multi-AZ**: Resources distributed across 3 availability zones
- **NAT Gateways**: High availability internet access for private subnets
- **Security Groups**: Minimal required access patterns

### EKS Configuration
- **Managed Node Groups**: Auto-scaling with desired/min/max capacity
- **Add-ons**: CoreDNS, kube-proxy, VPC-CNI, EBS CSI driver
- **Logging**: All control plane logs enabled
- **OIDC**: Service account integration ready

### Load Balancing
- **Application Load Balancer**: Layer 7 load balancing
- **Target Groups**: Health check configuration
- **SSL/TLS**: Automatic certificate management via ACM

## Validation Checklist

After deployment, verify:

```bash
# âœ… EKS Cluster Status
aws eks describe-cluster --name sre-challenge-staging --region us-east-1 --profile staging

# âœ… Node Groups
kubectl get nodes -o wide

# âœ… Cluster Services
kubectl get pods -n kube-system

# âœ… Load Balancer
aws elbv2 describe-load-balancers --region us-east-1 --profile staging

# âœ… Route53 Records
aws route53 list-hosted-zones --profile staging

# âœ… S3 Buckets
aws s3 ls --profile staging
```


# ğŸš€ Phase 2: Flux GitOps Setup

## ğŸ“‹ Overview

- **Repository Structure**: GitOps-ready with staging/production separation
- **Application**: nginx web server with health checks and auto-scaling
- **Monitoring**: Built-in health checks, HPA, and rollback capabilities
- **Security**: RBAC-enabled, namespace isolation, resource limits

## ğŸ¯ Installation Steps

### 1. Prerequisites

Ensure you have:
- âœ… EKS cluster running (from Phase 1)
- âœ… kubectl configured and working
- âœ… Flux CLI installed
- âœ… GitHub Personal Access Token with repo permissions

#### Install Flux CLI
```bash
brew install fluxcd/tap/flux
```

#### Create GitHub Token
1. Go to GitHub Settings > Developer settings > Personal access tokens
2. Create token with `repo` permissions
3. Export it: `export GITHUB_TOKEN=<your-token>`

### 2. Bootstrap Flux

Run the installation script:

```bash
./scripts/install-flux.sh <your-github-username> staging
```

This will:
- Install Flux controllers in `flux-system` namespace
- Configure GitRepository pointing to your repo
- Setup Kustomization to sync `clusters/staging`
- Commit Flux manifests to your repository

### 3. Verify Installation

```bash
# Check Flux status
flux get sources git
flux get kustomizations

# Check application deployment
kubectl get pods -n sre-challenge
kubectl get ingress -n sre-challenge
kubectl get hpa -n sre-challenge
```

## ğŸ”§ Application Features

### Health Checks
- **Liveness Probe**: HTTP GET on `/` every 10s
- **Readiness Probe**: HTTP GET on `/` every 5s  
- **Health Endpoint**: `/health` returns "healthy" status
- **Ingress Health**: ALB health checks on `/health` every 15s

### Auto-scaling
- **Min Replicas**: 2 (High Availability)
- **Max Replicas**: 10 (Burst capacity)
- **CPU Target**: 70% utilization
- **Memory Target**: 80% utilization
- **Scale-down**: 50% reduction every 60s (5min stabilization)
- **Scale-up**: 100% increase every 15s (1min stabilization)

### Security & Resource Management
- **Namespace Isolation**: `sre-challenge` namespace
- **Resource Requests**: 50m CPU, 64Mi memory
- **Resource Limits**: 100m CPU, 128Mi memory
- **RBAC**: Minimal required permissions
- **Image**: nginx:1.25-alpine (security-focused)

## ğŸ¨ Application Details

The sample application includes:
- **Modern UI**: Responsive design with glassmorphism effects
- **Environment Info**: Shows staging environment details
- **Health Status**: Visual confirmation of successful deployment
- **Version Tracking**: Displays deployment timestamp
- **Performance**: Optimized nginx configuration with gzip

## ğŸ”„ GitOps Workflow

### Making Changes
1. **Edit manifests** in `apps/staging/sre-challenge-app/`
2. **Commit & push** changes to repository
3. **Flux automatically syncs** within 1 minute
4. **Monitor deployment**: `flux get kustomizations --watch`

### Version Updates
Update the image tag in `deployment.yaml`:
```yaml
containers:
- name: nginx
  image: nginx:1.26-alpine  # Updated version
```

### Configuration Changes
Modify `configmap.yaml` to update:
- nginx configuration
- HTML content
- Application settings

## ğŸš¨ Automated Rollbacks

Flux provides automatic rollback capabilities:

### Health Check Failures
If pods fail health checks:
- **Readiness failures**: Traffic stops routing to failed pods
- **Liveness failures**: Kubernetes restarts failed pods
- **Multiple failures**: HPA may scale up additional replicas

### Deployment Failures
If deployment fails to roll out:
- **Flux reconciliation**: Continuously attempts to achieve desired state
- **Manual rollback**: `kubectl rollout undo deployment/sre-challenge-app -n sre-challenge`
- **Git revert**: Revert commit in Git repository for automatic rollback

### Configuration for Auto-rollback
Add to `deployment.yaml` spec:
```yaml
spec:
  progressDeadlineSeconds: 300
  revisionHistoryLimit: 5
  strategy:
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 25%
```

## ğŸ“Š Monitoring Commands

```bash
# Application Status
kubectl get pods -n sre-challenge
kubectl describe deployment sre-challenge-app -n sre-challenge
kubectl get hpa -n sre-challenge

# Flux Status
flux get kustomizations
flux get sources git
flux logs --follow

# Application Logs  
kubectl logs -l app=sre-challenge-app -n sre-challenge

# Ingress Status
kubectl get ingress -n sre-challenge
kubectl describe ingress sre-challenge-app -n sre-challenge
```

## ğŸŒ Accessing the Application

Once deployed, the application will be available at:
- **URL**: `https://eu-central-1.sre-challenge-panther.network`
- **Health Check**: `https://eu-central-1.sre-challenge-panther.network/health`

ğŸ’¡ Health Monitor Benefits:

  - ğŸ›¡ï¸ Zero-downtime deployments: Health checks prevent broken deployments
  - ğŸ”„ Automated validation: No manual testing needed after deployments
  - ğŸ“Š Deployment confidence: Clear success/failure feedback
  - ğŸš¨ Early failure detection: Catches issues before users notice
  - ğŸ§¹ Self-cleaning: TTL prevents job accumulation


## ğŸ”§ Troubleshooting

### Common Issues

**1. Flux not syncing**
```bash
flux reconcile source git flux-system
flux reconcile kustomization flux-system
flux reconcile kustomization -n sre-challenge
```

**2. Application not deploying**
```bash
kubectl describe kustomization flux-system -n flux-system
kubectl get events -n sre-challenge
```

**3. Ingress not working**
```bash
kubectl get ingress -n sre-challenge
kubectl describe ingress sre-challenge-app -n sre-challenge
```

**4. Pods not starting**
```bash
kubectl describe pod -l app=sre-challenge-app -n sre-challenge
kubectl logs -l app=sre-challenge-app -n sre-challenge
```

### Debug Commands
```bash
# Full Flux status
flux check
flux get all

# Application debugging
kubectl get all -n sre-challenge
kubectl describe deployment sre-challenge-app -n sre-challenge
```


The health monitor implements automated rollback validation - a critical SRE practice for zero-downtime deployments:

  Deployment Flow:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Git Push  â”‚ -> â”‚ Flux Detects â”‚ -> â”‚ Deploy App  â”‚ -> â”‚ Health Check â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                     â”‚
                                                              âœ… Pass â”‚ âŒ Fail
                                                                     â”‚
                                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                      â”‚        Auto Actions         â”‚
                                                      â”‚  âœ… Mark deployment Ready   â”‚
                                                      â”‚  âŒ Trigger rollback        â”‚
                                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Three-Layer Validation:

  1. Kubernetes Level: kubectl rollout status - Ensures deployment completed
  2. Pod Level: Counts ready vs total pods - Ensures all replicas healthy
  3. Application Level: HTTP health endpoint - Ensures app actually works                              

Failure Modes Caught:

  - ğŸš« Deployment stuck/failed
  - ğŸš« Pods crashlooping
  - ğŸš« App not responding to requests
  - ğŸš« Database connectivity issues
  - ğŸš« Configuration errors


Image Automation Architecture

  The three files create a complete automated container update pipeline:

  ğŸ“¦ imagerepository.yaml - Image Source Scanner


âº Purpose: Continuously scans Docker Hub for new nginx image tags
  Docker Hub Scanning:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Docker Hub  â”‚ <- â”‚   Flux Scan  â”‚ -> â”‚ Tag Cache   â”‚
  â”‚   nginx:*   â”‚    â”‚  Every 1min  â”‚    â”‚ 1.25-alpine â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  ğŸ¯ imagepolicy.yaml - Update Rules Engine

âº Purpose: Defines safe update rules using semantic versioning
  Update Policy Logic:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚Available    â”‚ -> â”‚ Filter Rules â”‚ -> â”‚ Approved    â”‚
  â”‚1.24.x       â”‚    â”‚>=1.25.0      â”‚    â”‚             â”‚
  â”‚1.25.5  âœ…   â”‚    â”‚<1.27.0       â”‚    â”‚ 1.25.5      â”‚
  â”‚1.26.2  âœ…   â”‚    â”‚*-alpine only â”‚    â”‚ 1.26.2      â”‚
  â”‚1.27.1  âŒ   â”‚    â”‚              â”‚    â”‚             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  Safety Features:
  - âœ… Patch updates: 1.25.4 -> 1.25.5 (bug fixes)
  - âœ… Minor updates: 1.25.x -> 1.26.x (new features)
  - âŒ Major updates: 1.25.x -> 2.x.x (breaking changes)
  - âœ… Alpine only: Consistent, secure base images

  ğŸ¤– imageupdateautomation.yaml - Git Update Robot

âº Purpose: Git robot that commits approved image updates back to the repository


Complete Automation Flow

  graph TD
      A[Docker Hub: nginx:1.25.5-alpine] -->|1min scan| B[ImageRepository]
      B --> C[ImagePolicy: Filter & Approve]
      C -->|Approved: 1.25.5| D[ImageUpdateAutomation]
      D -->|Every 30min| E[Git Commit: Update deployment.yaml]
      E --> F[Flux Detects Git Change]
      F --> G[Deploy New Image]
      G --> H[Health Monitor Job]
      H -->|Success| I[âœ… Deployment Complete]
      H -->|Failure| J[âŒ Auto Rollback]

  ğŸ”„ End-to-End Example:

  1. ğŸ³ New Image Available: nginx:1.25.5-alpine released
  2. ğŸ” ImageRepository: Scans and finds new tag
  3. âœ… ImagePolicy: Validates it meets criteria (>=1.25.0 <1.27.0 + alpine)
  4. ğŸ¤– ImageUpdateAutomation: Updates deployment.yaml and commits to Git
  5. âš¡ Flux Sync: Detects Git change and deploys new image
  6. ğŸ¥ Health Monitor: Runs post-deployment validation
  7. ğŸ¯ Result: Either âœ… Success or âŒ Auto-rollback

  ğŸ›¡ï¸ Safety Mechanisms:

  - Semantic Versioning: Only safe updates (no major version jumps)
  - Tag Filtering: Only alpine-based images (consistent + secure)
  - Health Validation: Application must be healthy after update
  - Auto-Rollback: Failed deployments automatically revert
  - Audit Trail: Every change has Git commit with full details

  ğŸ’¡ Business Value:

  - ğŸš€ Faster Security Patches: Auto-apply CVE fixes within 30 minutes
  - ğŸ›¡ï¸ Zero-Downtime Updates: Health checks prevent broken deployments
  - ğŸ“‹ Compliance: Full audit trail of all image changes
  - â° Reduced Toil: No manual monitoring of security updates
  - ğŸ¯ Reliability: Automatic rollback on failures

  This creates a self-healing, continuously updated infrastructure that maintains security and reliability while reducing operational overhead!


2. **Phase 3**: Monitoring and Logging - Setup Prometheus & Grafana  

This part covers the complete monitoring and logging setup for the SRE Challenge infrastructure, implementing observability best practices with Prometheus, Grafana, and Fluentd.

## Architecture

### Monitoring Stack
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Applications  â”‚â”€â”€â”€â–¶â”‚   Prometheus    â”‚â”€â”€â”€â–¶â”‚     Grafana     â”‚
â”‚   + Exporters   â”‚    â”‚   (Metrics)     â”‚    â”‚  (Dashboards)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
         â”‚              â”‚  AlertManager   â”‚              â”‚
         â”‚              â”‚   (Alerting)    â”‚              â”‚
         â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
         â”‚                                               â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚   Kubernetes    â”‚â”€â”€â”€â–¶â”‚     Fluent-bit  â”‚               â”‚
â”‚   Cluster Logs  â”‚    â”‚(Log Aggregation)â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
                                â”‚                        â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                       â”‚  Log Storage    â”‚               â”‚
                       â”‚   (Optional)    â”‚               â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
                                                         â”‚
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
                       â”‚   Ingress ALB   â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-â”˜
                       â”‚ grafana/prometheus.example.com â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

ğŸŒ Access URLs:

  - Prometheus: http://k8s-monitoringsimple-cc77efde23-614144927.eu-central-1.elb.amazonaws.com:9090/
  - Grafana: http://k8s-monitoringsimple-cc77efde23-614144927.eu-central-1.elb.amazonaws.com:3000/ (admin/admin123)


### Port Forwarding (Development)
```bash
# Grafana
kubectl port-forward -n monitoring svc/kube-prometheus-stack-grafana 3000:80

# Prometheus
kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090

# AlertManager
kubectl port-forward -n monitoring svc/kube-prometheus-stack-alertmanager 9093:9093
```

The application includes nginx-prometheus-exporter for detailed metrics:

**Metrics Endpoint**: `http://<endpoints>:9113/metrics`

**Available Metrics**:
- `nginx_connections_active` - Active connections
- `nginx_connections_reading` - Connections reading requests
- `nginx_connections_writing` - Connections writing responses
- `nginx_connections_waiting` - Idle connections
- `nginx_http_requests_total` - Total HTTP requests
- `nginx_up` - Nginx exporter status

### Custom Dashboard

The SRE Challenge Application Dashboard includes:
- **Application Availability**: Service uptime monitoring
- **CPU Usage**: Container CPU utilization
- **Memory Usage**: Container memory consumption
- **Pod Replicas**: Deployment replica status



## Log Analysis

### Fluent-bit Log Collection

**Log Format Enhancement**:
```json
{
  "log": "original log message",
  "stream": "stdout|stderr",
  "kubernetes": {
    "pod_name": "sre-challenge-app-xxx",
    "namespace_name": "sre-challenge",
    "container_name": "nginx"
  },
  "hostname": "node-name",
  "cluster_name": "sre-challenge",
  "environment": "staging"
}
```

### Log Query Examples

**View Application Logs**:
```bash
kubectl logs -n monitoring -l app=fluent-bit | grep sre-challenge-app
```

**Monitor Flux Logs**:
```bash
kubectl logs -n monitoring -l app=fluentd-bit | grep flux-system
```
---

**SRE Challenge - Complete! ğŸ‰**
SRE Challenge now has:
  - âœ… Full monitoring stack (Prometheus, Grafana, Fluent Bit)
  - âœ… Automated health validation with deployment verification
  - âœ… Robust GitOps workflow that doesn't hang
  - âœ… Application accessibility with proper ALB security groups
  - âœ… Automated rollback capability through health checks
