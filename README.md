# gitOps-workflow-flux

**Background -**
This repository contains my implementation of an SRE Challenge designed to demonstrate end-to-end skills in Kubernetes provisioning, 
GitOps workflows, application deployment, monitoring, and logging using Infrastructure as Code (IaC) principles.
The goal is to simulate a real-world Site Reliability Engineering scenario where an engineer is responsible for:
- Provisioning a secure and production-ready Kubernetes cluster using Terraform.
- Bootstrapping Flux for GitOps-based deployment automation.
- Deploying a sample web application with automated upgrades and rollbacks.
- Setting up basic monitoring (Prometheus, Grafana) and centralized logging (Fluentd).
- Following security, scalability, and resilience best practices.

This challenge emphasizes:
**Automation ‚Äî** All components can be deployed or destroyed without manual intervention.
**Security-first design ‚Äî** RBAC, Network Policies, and restricted Pod permissions.
**Operational excellence ‚Äî** Observability and fault-tolerance are built in from the start.

The documentation in this repository captures the steps taken, the issues encountered, and the design choices made ‚Äî making it reproducible and extensible for future improvements.

## üèóÔ∏è Repository Structure
```
‚îú‚îÄ‚îÄ apps
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ staging
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ sre-challenge-app
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ configmap.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ gitrepository.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ health-monitor.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ hpa.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ imagepolicy.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ imagerepository.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ imageupdateautomation.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ ingress.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ kustomization-flux.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ kustomization.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ namespace.yaml              # App namespace
‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ service.yaml
‚îú‚îÄ‚îÄ clusters
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ staging
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ flux-system
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ gotk-components.yaml        # Flux controllers
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ gotk-sync.yaml              # Git repository sync
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ kustomization.yaml          # Flux system config
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ image-automation.yaml
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ kustomization.yaml              # Cluster-level config
‚îú‚îÄ‚îÄ infrastructure
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ staging
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ aws-load-balancer-controller
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ crds.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ deployment.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ kustomization.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ namespace.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ rbac.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ serviceaccount.yaml
‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ webhook.yaml
‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ kustomization.yaml
‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ monitoring
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ application-health-dashboard.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ fluent-bit.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ kustomization.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ namespace.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ node-health-dashboard.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ prometheus-operator.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ servicemonitors.yaml
‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ simple-ingress.yaml
‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ sre-challenge-dashboard.yaml
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ scripts
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ deploy-global-regional.sh         # Terraform deployment script
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ install-flux.sh                   # Flux bootstrap script
‚îî‚îÄ‚îÄ terraform
    ‚îú‚îÄ‚îÄ environments
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ production
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ global
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ terraform.tfvars
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ regional
    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ eu-central-1
    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ terraform.tfvars
    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ us-east-1
    ‚îÇ¬†¬† ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ terraform.tfvars
    ‚îÇ¬†¬† ‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ staging
    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ global
    ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ backend-config.hcl
    ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ backend.hcl
    ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ terraform.tfvars
    ‚îÇ¬†¬†     ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ regional
    ‚îÇ¬†¬†         ‚îú‚îÄ‚îÄ eu-central-1
    ‚îÇ¬†¬†         ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ backend-config.hcl
    ‚îÇ¬†¬†         ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬†         ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬†         ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ terraform.tfvars
    ‚îÇ¬†¬†         ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬†         ‚îî‚îÄ‚îÄ us-east-1
    ‚îÇ¬†¬†             ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬†             ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬†             ‚îú‚îÄ‚îÄ terraform.tfvars
    ‚îÇ¬†¬†             ‚îî‚îÄ‚îÄ variables.tf
    ‚îú‚îÄ‚îÄ modules
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ acm
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ alb
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ eks
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ iam
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ iam-basic
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ iam-irsa
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ route53
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ route53-records
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ s3
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ variables.tf
    ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ vpc
    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ¬†¬†     ‚îú‚îÄ‚îÄ outputs.tf
    ‚îÇ¬†¬†     ‚îî‚îÄ‚îÄ variables.tf
    ‚îî‚îÄ‚îÄ README.md

```


# üöÄ Phase 1: Infrastructure Setup

## What Has Been Created

### üèóÔ∏è Terraform Modules
- **IAM Module**: EKS cluster roles, node group roles, AWS Load Balancer Controller, and Flux controller roles
- **VPC Module**: Multi-AZ VPC with public/private/database subnets, NAT gateways, security groups
- **EKS Module**: Managed Kubernetes cluster with node groups, addons, and OIDC provider
- **ALB Module**: Application Load Balancer with security groups and target groups
- **Route53 Module**: DNS management and SSL certificates via ACM
- **S3 Module**: Buckets for artifacts and configurations with lifecycle policies
- **ACM Module**: Automatic certificate management via ACM 


### üåç Multi-Region, Multi-Environment Structure
```
Staging Account (215876814712):
‚îú‚îÄ‚îÄ us-east-1    (VPC: 10.0.0.0/16)
‚îî‚îÄ‚îÄ eu-central-1 (VPC: 10.0.0.0/16)

Production Account (746848447423):
‚îú‚îÄ‚îÄ us-east-1    (VPC: 10.10.0.0/16)
‚îî‚îÄ‚îÄ eu-central-1 (VPC: 10.20.0.0/16)
```

### üîí Security Features Implemented
- **Network Isolation**: Private subnets for EKS nodes, security groups with minimal access
- **RBAC**: Kubernetes role-based access control enabled
- **Encryption**: KMS keys for EKS secrets and S3 buckets
- **IAM**: Least privilege roles with OIDC integration
- **Monitoring**: CloudWatch logging and VPC flow logs

## Quick Start Deployment

### Prerequisites
1. AWS CLI configured with profiles:
   ```bash
   aws configure --profile staging    # Account: 215876814712
   aws configure --profile production # Account: 746848447423
   ```

2. Terraform >= 1.0 installed
3. kubectl installed

### Step 1: Deploy Infrastructure
```bash
# Deploy staging infrastructure
./scripts/deploy-global-regional.sh staging eu-central-1 apply # this is the operational region for this POC
./scripts/deploy-global-regional.sh staging us-east-1 apply # Nearly equivalent but not tested

# Deploy production infrastructure
./scripts/deploy-global-regional.sh production us-east-1 apply # Nearly equivalent but not tested
./scripts/deploy-global-regional.sh production eu-central-1 apply # Nearly equivalent but not tested
```

### Step 2: Configure kubectl
```bash
# Connect to staging cluster
aws eks update-kubeconfig --region us-east-1 --name sre-challenge-staging --profile staging

# Verify connection
kubectl cluster-info
kubectl get nodes
```

## Environment Configurations

### Staging Environment
- **Deletion Protection**: Disabled (easy cleanup)
- **Log Retention**: 7 days
- **Node Groups**: Includes spot instances for cost optimization
- **Force Destroy**: Enabled for development workflows
- **Domain**: sre-challenge-panther.network

### Production Environment
- **TODO**

## Architecture Overview

### Network Design
- **3-Tier Architecture**: Public, Private, Database subnets
- **Multi-AZ**: Resources distributed across 3 availability zones
- **NAT Gateways**: High availability internet access for private subnets
- **Security Groups**: Minimal required access patterns

### EKS Configuration
- **Managed Node Groups**: Auto-scaling with desired/min/max capacity
- **Add-ons**: CoreDNS, kube-proxy, VPC-CNI, EBS CSI driver
- **Logging**: All control plane logs enabled
- **OIDC**: Service account integration ready

### Load Balancing
- **Application Load Balancer**: Layer 7 load balancing
- **Target Groups**: Health check configuration
- **SSL/TLS**: Automatic certificate management via ACM

## Validation Checklist

After deployment, verify:

```bash
# ‚úÖ EKS Cluster Status
aws eks describe-cluster --name sre-challenge-staging --region us-east-1 --profile staging

# ‚úÖ Node Groups
kubectl get nodes -o wide

# ‚úÖ Cluster Services
kubectl get pods -n kube-system

# ‚úÖ Load Balancer
aws elbv2 describe-load-balancers --region us-east-1 --profile staging

# ‚úÖ Route53 Records
aws route53 list-hosted-zones --profile staging

# ‚úÖ S3 Buckets
aws s3 ls --profile staging
```


# üöÄ Phase 2: Flux GitOps Setup

## üìã Overview

- **Repository Structure**: GitOps-ready with staging/production separation
- **Application**: nginx web server with health checks and auto-scaling
- **Monitoring**: Built-in health checks, HPA, and rollback capabilities
- **Security**: RBAC-enabled, namespace isolation, resource limits

## üéØ Installation Steps

### 1. Prerequisites

Ensure you have:
- ‚úÖ EKS cluster running (from Phase 1)
- ‚úÖ kubectl configured and working
- ‚úÖ Flux CLI installed
- ‚úÖ GitHub Personal Access Token with repo permissions

#### Install Flux CLI
```bash
brew install fluxcd/tap/flux
```

#### Create GitHub Token
1. Go to GitHub Settings > Developer settings > Personal access tokens
2. Create token with `repo` permissions
3. Export it: `export GITHUB_TOKEN=<your-token>`

### 2. Bootstrap Flux

Run the installation script:

```bash
./scripts/install-flux.sh <your-github-username> staging
```

This will:
- Install Flux controllers in `flux-system` namespace
- Configure GitRepository pointing to your repo
- Setup Kustomization to sync `clusters/staging`
- Commit Flux manifests to your repository

### 3. Verify Installation

```bash
# Check Flux status
flux get sources git
flux get kustomizations

# Check application deployment
kubectl get pods -n sre-challenge
kubectl get ingress -n sre-challenge
kubectl get hpa -n sre-challenge
```

## üîß Application Features

### Health Checks
- **Liveness Probe**: HTTP GET on `/` every 10s
- **Readiness Probe**: HTTP GET on `/` every 5s  
- **Health Endpoint**: `/health` returns "healthy" status
- **Ingress Health**: ALB health checks on `/health` every 15s

### Auto-scaling
- **Min Replicas**: 2 (High Availability)
- **Max Replicas**: 10 (Burst capacity)
- **CPU Target**: 70% utilization
- **Memory Target**: 80% utilization
- **Scale-down**: 50% reduction every 60s (5min stabilization)
- **Scale-up**: 100% increase every 15s (1min stabilization)

### Security & Resource Management
- **Namespace Isolation**: `sre-challenge` namespace
- **Resource Requests**: 50m CPU, 64Mi memory
- **Resource Limits**: 100m CPU, 128Mi memory
- **RBAC**: Minimal required permissions
- **Image**: nginx:1.25-alpine (security-focused)

## üé® Application Details

The sample application includes:
- **Modern UI**: Responsive design with glassmorphism effects
- **Environment Info**: Shows staging environment details
- **Health Status**: Visual confirmation of successful deployment
- **Version Tracking**: Displays deployment timestamp
- **Performance**: Optimized nginx configuration with gzip

## üîÑ GitOps Workflow

### Making Changes
1. **Edit manifests** in `apps/staging/sre-challenge-app/`
2. **Commit & push** changes to repository
3. **Flux automatically syncs** within 1 minute
4. **Monitor deployment**: `flux get kustomizations --watch`

### Version Updates
Update the image tag in `deployment.yaml`:
```yaml
containers:
- name: nginx
  image: nginx:1.26-alpine  # Updated version
```

### Configuration Changes
Modify `configmap.yaml` to update:
- nginx configuration
- HTML content
- Application settings

## üö® Automated Rollbacks

Flux provides automatic rollback capabilities:

### Health Check Failures
If pods fail health checks:
- **Readiness failures**: Traffic stops routing to failed pods
- **Liveness failures**: Kubernetes restarts failed pods
- **Multiple failures**: HPA may scale up additional replicas

### Deployment Failures
If deployment fails to roll out:
- **Flux reconciliation**: Continuously attempts to achieve desired state
- **Manual rollback**: `kubectl rollout undo deployment/sre-challenge-app -n sre-challenge`
- **Git revert**: Revert commit in Git repository for automatic rollback

### Configuration for Auto-rollback
Add to `deployment.yaml` spec:
```yaml
spec:
  progressDeadlineSeconds: 300
  revisionHistoryLimit: 5
  strategy:
    rollingUpdate:
      maxSurge: 50%
      maxUnavailable: 25%
```

## üìä Monitoring Commands

```bash
# Application Status
kubectl get pods -n sre-challenge
kubectl describe deployment sre-challenge-app -n sre-challenge
kubectl get hpa -n sre-challenge

# Flux Status
flux get kustomizations
flux get sources git
flux logs --follow

# Application Logs  
kubectl logs -l app=sre-challenge-app -n sre-challenge

# Ingress Status
kubectl get ingress -n sre-challenge
kubectl describe ingress sre-challenge-app -n sre-challenge
```

## üåê Accessing the Application

Once deployed, the application will be available at:
- **URL**: `https://eu-central-1.sre-challenge-panther.network`
- **Health Check**: `https://eu-central-1.sre-challenge-panther.network/health`

üí° Health Monitor Benefits:

  - üõ°Ô∏è Zero-downtime deployments: Health checks prevent broken deployments
  - üîÑ Automated validation: No manual testing needed after deployments
  - üìä Deployment confidence: Clear success/failure feedback
  - üö® Early failure detection: Catches issues before users notice
  - üßπ Self-cleaning: TTL prevents job accumulation


## üîß Troubleshooting

### Common Issues

**1. Flux not syncing**
```bash
flux reconcile source git flux-system
flux reconcile kustomization flux-system
flux reconcile kustomization -n sre-challenge
```

**2. Application not deploying**
```bash
kubectl describe kustomization flux-system -n flux-system
kubectl get events -n sre-challenge
```

**3. Ingress not working**
```bash
kubectl get ingress -n sre-challenge
kubectl describe ingress sre-challenge-app -n sre-challenge
```

**4. Pods not starting**
```bash
kubectl describe pod -l app=sre-challenge-app -n sre-challenge
kubectl logs -l app=sre-challenge-app -n sre-challenge
```

### Debug Commands
```bash
# Full Flux status
flux check
flux get all

# Application debugging
kubectl get all -n sre-challenge
kubectl describe deployment sre-challenge-app -n sre-challenge
```




2. **Phase 4**: Monitoring and Logging - Setup Prometheus & Grafana  

---

**SRE Challenge - Complete! üéâ**
SRE Challenge now has:
  - ‚úÖ Full monitoring stack (Prometheus, Grafana, Fluent Bit)
  - ‚úÖ Automated health validation with deployment verification
  - ‚úÖ Robust GitOps workflow that doesn't hang
  - ‚úÖ Application accessibility with proper ALB security groups
  - ‚úÖ Automated rollback capability through health checks
