apiVersion: v1
kind: ServiceAccount
metadata:
  name: health-checker
  namespace: sre-challenge
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: health-checker
  namespace: sre-challenge
rules:
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["get", "list"]
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: health-checker
  namespace: sre-challenge
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: health-checker
subjects:
- kind: ServiceAccount
  name: health-checker
  namespace: sre-challenge
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: health-monitor-config
  namespace: sre-challenge
data:
  monitor.sh: |
    #!/bin/sh
    # Health monitoring script for automated rollback validation
    
    APP_NAME="sre-challenge-app"
    NAMESPACE="sre-challenge"
    SERVICE_URL="http://sre-challenge-app.sre-challenge.svc.cluster.local:80/health"
    
    echo "Starting health monitoring for $APP_NAME..."
    
    # Check deployment rollout status
    if ! kubectl rollout status deployment/$APP_NAME -n $NAMESPACE --timeout=300s; then
        echo "ERROR: Deployment rollout failed"
        exit 1
    fi
    
    # Check all pods are ready
    READY_PODS=$(kubectl get pods -n $NAMESPACE -l app=$APP_NAME -o jsonpath='{.items[*].status.conditions[?(@.type=="Ready")].status}' | grep -o True | wc -l)
    TOTAL_PODS=$(kubectl get pods -n $NAMESPACE -l app=$APP_NAME --no-headers | wc -l)
    
    if [ "$READY_PODS" -ne "$TOTAL_PODS" ]; then
        echo "ERROR: Not all pods are ready. Ready: $READY_PODS, Total: $TOTAL_PODS"
        exit 1
    fi
    
    # Internal service health check
    i=1
    while [ $i -le 10 ]; do
        if curl -f -s -o /dev/null "$SERVICE_URL"; then
            echo "SUCCESS: Application is healthy and responding"
            exit 0
        fi
        echo "Attempt $i/10: Health check failed, retrying in 5s..."
        sleep 5
        i=$((i + 1))
    done
    
    echo "ERROR: Application failed health checks after 10 attempts"
    exit 1
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: health-monitor-cronjob
  namespace: sre-challenge
  annotations:
    # Continuous health monitoring that doesn't block Flux reconciliation
    kustomize.toolkit.fluxcd.io/reconcile: "disabled"
spec:
  # Run health check every 5 minutes
  schedule: "*/5 * * * *"
  successfulJobsHistoryLimit: 2
  failedJobsHistoryLimit: 2
  jobTemplate:
    spec:
      # Clean up job after completion
      ttlSecondsAfterFinished: 300
      template:
        spec:
          serviceAccountName: health-checker
          containers:
          - name: health-checker
            image: alpine/curl:latest
            command: ["/bin/sh"]
            args: ["-c", "apk add --no-cache kubectl && /scripts/monitor.sh"]
            volumeMounts:
            - name: scripts
              mountPath: /scripts
            env:
            - name: KUBECONFIG
              value: "/tmp/kubeconfig"
          volumes:
          - name: scripts
            configMap:
              name: health-monitor-config
              defaultMode: 0755
          restartPolicy: Never
      backoffLimit: 1